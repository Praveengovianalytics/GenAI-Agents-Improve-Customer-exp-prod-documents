{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNaIWgJ1Pz64MSzeeFkOMQ1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tc4LVpk1ZjRh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install the necessary libraries"
      ],
      "metadata": {
        "id": "3p8mJn1pZj5S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ajumo4lsZUVV",
        "outputId": "546a857e-3697-42ec-ee30-12a4c1c2e7f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m817.7/817.7 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.0/64.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.3/299.3 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.4/116.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m312.3/312.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.8/30.8 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU langchain langchain_openai langgraph arxiv duckduckgo-search -q\n",
        "!pip install -qU faiss-cpu pymupdf pypdf -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I1IwUIjlc0YR"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from uuid import uuid4\n",
        "import os\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import ArxivLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai import OpenAIEmbeddings"
      ],
      "metadata": {
        "id": "n19zWYv_Zdpk"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIE1 - LangGraph - {uuid4().hex[0:8]}\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] =  userdata.get('LANGCHAIN_API_KEY')"
      ],
      "metadata": {
        "id": "pYUcmDUjZdsp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instantiate a Simple Retrieval Chain using LCEL"
      ],
      "metadata": {
        "id": "xujQfob-at-m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loader = PyPDFLoader(\"course-catalog.pdf\")\n",
        "pages = loader.load_and_split()"
      ],
      "metadata": {
        "id": "VK9V4ryrdJDj"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dpocument into smaller chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=350, chunk_overlap=50\n",
        ")\n",
        "\n",
        "chunked_documents = text_splitter.split_documents(pages)\n",
        "#\n",
        "# Instantiate the Embedding Model\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\",openai_api_key=os.environ['OPENAI_API_KEY'])\n",
        "# Create Index- Load document chunks into the vectorstore\n",
        "faiss_vectorstore = FAISS.from_documents(\n",
        "    documents=chunked_documents,\n",
        "    embedding=embeddings,\n",
        ")\n",
        "# Create a retriver\n",
        "retriever = faiss_vectorstore.as_retriever()"
      ],
      "metadata": {
        "id": "nNy94-_CZdvT"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate RAG prompt"
      ],
      "metadata": {
        "id": "VKEpd_lxa8R5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "RAG_PROMPT = \"\"\"\\\n",
        "Use the following context to answer the user's query. If you cannot answer the question, please respond with 'I don't know'.\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT)"
      ],
      "metadata": {
        "id": "-o9FkF0YZd0x"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instantiate the LLM"
      ],
      "metadata": {
        "id": "MCdnPKF1bGKI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "openai_chat_model = ChatOpenAI(model=\"gpt-3.5-turbo\")"
      ],
      "metadata": {
        "id": "rqH-Kp9AZd3k"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "\n",
        "retrieval_augmented_generation_chain = (\n",
        "       {\"context\": itemgetter(\"question\")\n",
        "    | retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | openai_chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ],
      "metadata": {
        "id": "ndP7XJcKZd6N"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieval_augmented_generation_chain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqNEVj60b0uR",
        "outputId": "d577ab77-afa0-4607-d2b0-a9d9eea6c5ce"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{\n",
              "  context: RunnableLambda(itemgetter('question'))\n",
              "           | VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7c3f08612e30>),\n",
              "  question: RunnableLambda(itemgetter('question'))\n",
              "}\n",
              "| RunnableAssign(mapper={\n",
              "    context: RunnableLambda(itemgetter('context'))\n",
              "  })\n",
              "| {\n",
              "    response: ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"Use the following context to answer the user's query. If you cannot answer the question, please respond with 'I don't know'.\\n\\nQuestion:\\n{question}\\n\\nContext:\\n{context}\\n\"))])\n",
              "              | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7c3f0877b6a0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7c3f08772230>, openai_api_key=SecretStr('**********'), openai_proxy=''),\n",
              "    context: RunnableLambda(itemgetter('context'))\n",
              "  }"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "await retrieval_augmented_generation_chain.ainvoke({\"question\" : \"How can i get details about GCP databricks cloud integrations \"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvQBKZV8b2nU",
        "outputId": "e5de47d5-04df-4e49-f446-cd459daeb417"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'response': AIMessage(content=\"To get details about GCP Databricks cloud integrations, you can enroll in the course provided in the document. The course covers topics like deploying workspaces, custom-managed keys, encryption levels, creating GCP buckets, setting up Google Pub/Sub, and more. It also requires beginner-level knowledge of GCP and access to a GCP project. \\n\\nI don't know where else you could find this information.\", response_metadata={'token_usage': {'completion_tokens': 85, 'prompt_tokens': 1172, 'total_tokens': 1257}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3b956da36b', 'finish_reason': 'stop', 'logprobs': None}, id='run-50e6db25-8366-4150-8538-8d7849dda666-0'),\n",
              " 'context': [Document(page_content='●\\nDeploy\\nworkspaces\\ninto\\nyour\\nown\\nmanaged\\nVPCs.\\n●\\nCreate\\nyour\\nown\\ncustomer-managed\\nkeys.\\n●\\nApply\\ncustomer-managed\\nkeys\\nto\\nachieve\\ndifferent\\nlevels\\nof\\nencryption\\nin\\n●\\nyour\\nDatabricks\\nworkspaces.\\nGCP\\nDatabricks\\nCloud\\nIntegrations\\nClick\\nhere\\nfor\\nthe\\ncustomer\\nenrollment\\nlink.\\nDuration:\\n1\\nhour\\nCourse\\ndescription:\\nThis\\nvideo\\nseries\\nwill\\nprovide\\nyou\\nwith\\nthe\\nbackground\\nneeded\\nto\\ncustomize\\nthe\\nstructure\\nof\\nyour\\nenvironment\\nand\\nreinforce\\nsecurity\\nat\\nthe\\ninfrastructural\\nlevel.\\nPrerequisites:\\n●\\nBeginner-level\\nknowledge\\nof\\nGCP\\n●\\nAccount\\nadministrator\\ncapabilities\\nin\\nyour\\nDatabricks\\naccount\\n●\\nAccess\\nto\\nyour\\na\\nGCP\\nproject,\\nwith\\nthe\\nability\\nto\\nenable\\nAPIs\\nand\\ncreate\\nservice\\naccounts\\nand\\nbuckets\\nLearning\\nobjectives:\\n●\\nCreate\\nyour\\nown\\nexternal\\nGCP\\nbucket\\nand\\naccess\\ndata\\nfrom\\nDatabricks.\\n●\\nSet\\nup\\na\\ntopic\\nand\\nsubscription\\nin\\nGoogle\\nPub/Sub\\n(Lite)\\nand\\nstream\\nmessages\\nfrom\\nDatabricks.\\n●\\nSet\\nup\\na\\ndata\\nwarehouse\\nin\\nGoogle', metadata={'source': 'course-catalog.pdf', 'page': 36}),\n",
              "  Document(page_content='Course\\ndescription:\\nThis\\nvideo\\nseries\\nwill\\nprovide\\nyou\\nwith\\na\\nhigh-level\\noverview\\nof\\nthe\\nGoogle\\nCloud\\nPlatform\\n(GCP)\\nenvironment\\nas\\nit\\nrelates\\nto\\nDatabricks,\\nand\\nit\\nwill\\nguide\\nyou\\nthrough\\nhow\\nto\\nperform\\nsome\\nfundamental\\ntasks\\nrelated\\nto\\ndeploying\\nand\\nmanaging\\nDatabricks\\nworkspaces\\nin\\nyour\\norganization\\nthrough\\nGCP.\\nPrerequisites:\\n●\\nBeginner-level\\nknowledge\\nof\\nGCP\\n●\\nBeginner-level\\nknowledge\\nof\\nTerraform\\nis\\nhelpful\\n●\\nAccess\\nto\\na\\nGCP\\nproject\\nis\\nrequired,\\nwith\\nthe\\nability\\nto\\nadd\\nprincipals\\nand\\nservice\\naccounts\\n●\\nAccount\\nadministrator\\ncapabilities\\nin\\nyour\\nDatabricks\\naccount\\nLearning\\nobjectives:\\n●\\nDescribe\\nand\\nidentify\\nelements\\nof\\nthe\\nGCP\\nDatabricks\\narchitecture\\n●\\nCreate\\nand\\nmanage\\nworkspaces\\nand\\nmetastores,\\nand\\nsupporting\\nresources\\n●\\nAutomate\\nadministration\\noperations\\nGCP\\nDatabricks\\nNetworking\\nand\\nSecurity\\nFundamentals\\nClick\\nhere\\nfor\\nthe\\ncustomer\\nenrollment\\nlink.\\nDuration:\\n1\\nhour\\nCourse\\ndescription:\\nThis\\nvideo\\nseries\\nwill\\nprovide\\nyou\\nwith', metadata={'source': 'course-catalog.pdf', 'page': 35}),\n",
              "  Document(page_content='account\\nteam\\nthat\\nwill\\nbe\\ninteracting\\nwith\\nyou\\nthroughout\\nyour\\ncustomer\\njourney.\\n●\\nLocate\\nwhere\\nto\\nﬁnd\\nfurther\\ninformation\\nand\\ntraining\\non\\nthe\\nDatabricks\\nLakehouse\\nPlatform.\\n●\\nDescribe\\nhow\\nDatabricks\\npromotes\\na\\nsecure\\ndata\\nenvironment\\nthat\\ncan\\nbe\\neasily\\ngoverned\\nand\\nscaled.\\n●\\nExplain\\nhow,\\nas\\nan\\norganization\\nusing\\nDatabricks,\\nyour\\ncompany\\nwill\\nbe\\nable\\nto\\nreduce\\ntheir\\ntotal\\ncost\\nof\\nownership\\nof\\ndata\\nmanagement\\nsolutions\\nby\\nusing\\nDatabricks.\\n●\\nDeﬁne\\ncommon\\ndata\\nscience\\nterms\\nused\\nby\\nDatabricks\\nwhen\\ndiscussing\\nthe\\nDatabricks\\nLakehouse\\nPlatform.\\nGCP\\nDatabricks\\nPlatform\\nAdministration\\nFundamentals\\nClick\\nhere\\nfor\\nthe\\ncustomer\\nenrollment\\nlink.\\nDuration:\\n1.30\\nhours', metadata={'source': 'course-catalog.pdf', 'page': 34}),\n",
              "  Document(page_content='Platform\\nAdministration\\nFundamentals\\n35\\nGCP\\nDatabricks\\nNetworking\\nand\\nSecurity\\nFundamentals\\n36\\nGCP\\nDatabricks\\nCloud\\nIntegrations\\n37\\nGenerative\\nAI\\nFundamentals\\n37\\nIncremental\\nProcessing\\nwith\\nSpark\\nStructured\\nStreaming\\n38\\nIntroduction\\nto\\nDelta\\nSharing\\n39', metadata={'source': 'course-catalog.pdf', 'page': 1})]}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating our Tool Belt"
      ],
      "metadata": {
        "id": "KLampJoJfxpf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As is usually the case, we’ll want to equip our agent with a toolbelt to help answer questions and add external knowledge.\n",
        "\n",
        "There’s a load of tools in the LangChain Community Repo but we’ll stick to a couple just so we can observe the cyclic nature of LangGraph in action!"
      ],
      "metadata": {
        "id": "aZwvL1oDf8XE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.tools.ddg_search import DuckDuckGoSearchRun\n",
        "from langgraph.prebuilt import ToolExecutor\n",
        "tool_belt = [\n",
        "    DuckDuckGoSearchRun()\n",
        "]\n",
        "\n",
        "tool_executor = ToolExecutor(tool_belt)"
      ],
      "metadata": {
        "id": "sp0M-i09cIwm"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.utils.function_calling import convert_to_openai_function\n",
        "\n",
        "model = ChatOpenAI(temperature=0)\n",
        "\n",
        "functions = [convert_to_openai_function(t) for t in tool_belt]\n",
        "\n",
        "model = model.bind_functions(functions)"
      ],
      "metadata": {
        "id": "kIrv02c-gGKV"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Leverage LangGraph\n",
        "\n",
        "LangGraph leverages a StatefulGraph which uses an AgentState object to pass information between the various nodes of the graph.\n",
        "\n",
        "There are more options than what we’ll see below — but this AgentState object is one that is stored in a TypedDict with the key messages and the value is a Sequence of BaseMessages that will be appended to whenever the state changes."
      ],
      "metadata": {
        "id": "DGolxlNAgXwI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypedDict, Annotated, Sequence\n",
        "import operator\n",
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[Sequence[BaseMessage], operator.add]"
      ],
      "metadata": {
        "id": "oEKCLrxFgRjL"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.prebuilt import ToolInvocation\n",
        "import json\n",
        "from langchain_core.messages import FunctionMessage\n",
        "\n",
        "def call_model(state):\n",
        "  messages = state[\"messages\"]\n",
        "  response = model.invoke(messages)\n",
        "  return {\"messages\" : [response]}\n",
        "\n",
        "def call_tool(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  action = ToolInvocation(\n",
        "      tool=last_message.additional_kwargs[\"function_call\"][\"name\"],\n",
        "      tool_input=json.loads(\n",
        "          last_message.additional_kwargs[\"function_call\"][\"arguments\"]\n",
        "      )\n",
        "  )\n",
        "\n",
        "  response = tool_executor.invoke(action)\n",
        "\n",
        "  function_message = FunctionMessage(content=str(response), name=action.tool)\n",
        "\n",
        "  return {\"messages\" : [function_message]}"
      ],
      "metadata": {
        "id": "yYvzQ8eKgeLz"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Workflow"
      ],
      "metadata": {
        "id": "t0TEcG4-gm-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "workflow.add_node(\"agent\", call_model)\n",
        "workflow.add_node(\"action\", call_tool)\n",
        "workflow.nodes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wi-T0C4gjXH",
        "outputId": "10a74faa-d1ed-4b49-fb10-6042db38ddfc"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'agent': RunnableLambda(call_model), 'action': RunnableLambda(call_tool)}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "workflow.set_entry_point(\"agent\")"
      ],
      "metadata": {
        "id": "AfbbMdQvgps-"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conditional edge for routing"
      ],
      "metadata": {
        "id": "djQ5YQz7gxef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def should_continue(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  if \"function_call\" not in last_message.additional_kwargs:\n",
        "    return \"end\"\n",
        "\n",
        "  return \"continue\"\n",
        "\n",
        "workflow.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    should_continue,\n",
        "    {\n",
        "        \"continue\" : \"action\",\n",
        "        \"end\" : END\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "Kf-3Jyhugwc7"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally connect the conditional edge to the agent node and action node"
      ],
      "metadata": {
        "id": "QcFxnSOxg_Rv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "workflow.add_edge(\"action\", \"agent\")"
      ],
      "metadata": {
        "id": "n3ML8CNPgwaj"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "app = workflow.compile()\n",
        "#\n",
        "app"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtYdI1degwYG",
        "outputId": "d6ede1fb-62d1-48cf-d382-f4ecd42aab2c"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CompiledStateGraph(nodes={'__start__': PregelNode(config={'tags': ['langsmith:hidden']}, channels=['__start__'], triggers=['__start__'], writers=[ChannelWrite<messages>(recurse=True, writes=[ChannelWriteEntry(channel='messages', value=<object object at 0x7c3f0c2cff00>, skip_none=False, mapper=_get_state_key(recurse=False))]), ChannelWrite<start:agent>(recurse=True, writes=[ChannelWriteEntry(channel='start:agent', value='__start__', skip_none=False, mapper=None)])]), 'agent': PregelNode(config={'tags': []}, channels={'messages': 'messages'}, triggers=['action', 'start:agent'], mapper=functools.partial(<function _coerce_state at 0x7c3f0887fd90>, <class '__main__.AgentState'>), writers=[ChannelWrite<agent,messages>(recurse=True, writes=[ChannelWriteEntry(channel='agent', value='agent', skip_none=False, mapper=None), ChannelWriteEntry(channel='messages', value=<object object at 0x7c3f0c2cff00>, skip_none=False, mapper=_get_state_key(recurse=False))]), _route(recurse=True, _is_channel_writer=True)]), 'action': PregelNode(config={'tags': []}, channels={'messages': 'messages'}, triggers=['branch:agent:should_continue:action'], mapper=functools.partial(<function _coerce_state at 0x7c3f0887fd90>, <class '__main__.AgentState'>), writers=[ChannelWrite<action,messages>(recurse=True, writes=[ChannelWriteEntry(channel='action', value='action', skip_none=False, mapper=None), ChannelWriteEntry(channel='messages', value=<object object at 0x7c3f0c2cff00>, skip_none=False, mapper=_get_state_key(recurse=False))])])}, channels={'messages': <langgraph.channels.binop.BinaryOperatorAggregate object at 0x7c3f085766e0>, '__start__': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x7c3f0855ea40>, 'agent': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x7c3f0855ec80>, 'action': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x7c3f08576140>, 'start:agent': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x7c3f0855e650>, 'branch:agent:should_continue:action': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x7c3f085775b0>}, auto_validate=False, stream_mode='updates', output_channels=['messages'], stream_channels=['messages'], input_channels='__start__', graph=<langgraph.graph.state.StateGraph object at 0x7c3f08575240>)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "inputs = {\"messages\" : [HumanMessage(content=\"What is pre requisite for Generative AI fundamendals\")]}\n",
        "\n",
        "response = app.invoke(inputs)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zghq0-QMhHDe",
        "outputId": "9f69717c-e97c-4ea0-c7c7-a19046820d70"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'messages': [HumanMessage(content='What is pre requisite for Generative AI fundamendals'), AIMessage(content='The prerequisites for learning Generative AI fundamentals typically include a strong understanding of the following topics:\\n\\n1. Machine Learning: Knowledge of machine learning concepts and algorithms is essential for understanding how generative models work.\\n\\n2. Deep Learning: Familiarity with deep learning frameworks like TensorFlow or PyTorch, as well as neural networks, is important for implementing generative models.\\n\\n3. Probability and Statistics: Understanding probability theory and statistical concepts is crucial for working with generative models and evaluating their performance.\\n\\n4. Linear Algebra: Proficiency in linear algebra is necessary for understanding the mathematical foundations of generative models.\\n\\n5. Python Programming: Proficiency in Python programming is essential for implementing generative models using popular libraries like TensorFlow, PyTorch, or Keras.\\n\\n6. Data Preprocessing: Knowledge of data preprocessing techniques is important for preparing and cleaning data before training generative models.\\n\\n7. Computer Vision or Natural Language Processing: Depending on the application domain, familiarity with computer vision or natural language processing concepts may be required for working with generative models in specific tasks.\\n\\nThese are some of the key prerequisites for learning Generative AI fundamentals. It is recommended to have a solid foundation in these areas before diving into the study of generative models.', response_metadata={'token_usage': {'completion_tokens': 248, 'prompt_tokens': 88, 'total_tokens': 336}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3b956da36b', 'finish_reason': 'stop', 'logprobs': None}, id='run-7af2468d-4930-4a8e-9ee3-e551c9b20738-0')]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response['messages'][-1].content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "3USh53gshVql",
        "outputId": "84ceaea1-cda3-4e6e-b7d3-6fcf2ffe26b9"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The prerequisites for learning Generative AI fundamentals typically include a strong understanding of the following topics:\\n\\n1. Machine Learning: Knowledge of machine learning concepts and algorithms is essential for understanding how generative models work.\\n\\n2. Deep Learning: Familiarity with deep learning frameworks like TensorFlow or PyTorch, as well as neural networks, is important for implementing generative models.\\n\\n3. Probability and Statistics: Understanding probability theory and statistical concepts is crucial for working with generative models and evaluating their performance.\\n\\n4. Linear Algebra: Proficiency in linear algebra is necessary for understanding the mathematical foundations of generative models.\\n\\n5. Python Programming: Proficiency in Python programming is essential for implementing generative models using popular libraries like TensorFlow, PyTorch, or Keras.\\n\\n6. Data Preprocessing: Knowledge of data preprocessing techniques is important for preparing and cleaning data before training generative models.\\n\\n7. Computer Vision or Natural Language Processing: Depending on the application domain, familiarity with computer vision or natural language processing concepts may be required for working with generative models in specific tasks.\\n\\nThese are some of the key prerequisites for learning Generative AI fundamentals. It is recommended to have a solid foundation in these areas before diving into the study of generative models.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MbfkxYQmhdKw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}